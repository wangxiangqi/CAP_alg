{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         y      x      z       a    u\n",
      "2378   0.0  126.0  109.0   326.0  1.0\n",
      "376    4.0  124.0  102.0   588.0  2.0\n",
      "1948   6.0  128.0  105.0  1537.0  2.0\n",
      "3893   3.0  118.0  116.0   494.0  3.0\n",
      "568    8.0  117.0  116.0  1096.0  3.0\n",
      "...    ...    ...    ...     ...  ...\n",
      "3444   4.0  118.0  110.0   739.0  3.0\n",
      "466   15.0  111.0   97.0  1686.0  3.0\n",
      "3092   2.0  125.0  108.0   375.0  1.0\n",
      "3772  14.0  114.0   78.0  1940.0  3.0\n",
      "860   12.0  128.0   79.0  1973.0  3.0\n",
      "\n",
      "[3924 rows x 5 columns]\n",
      "         y      x      z       a    u\n",
      "179    4.0  126.0  100.0   505.0  1.0\n",
      "3166   2.0  119.0  112.0   637.0  3.0\n",
      "2740  10.0  119.0   96.0  1092.0  3.0\n",
      "2191   6.0  121.0  109.0   628.0  2.0\n",
      "1578   6.0  120.0  109.0   540.0  1.0\n",
      "...    ...    ...    ...     ...  ...\n",
      "596    4.0  108.0  123.0   634.0  4.0\n",
      "911    0.0  124.0  108.0   305.0  1.0\n",
      "4034  11.0  116.0  108.0  1038.0  3.0\n",
      "1626   8.0  113.0   89.0  1099.0  2.0\n",
      "2099  14.0  121.0  108.0  2310.0  2.0\n",
      "\n",
      "[437 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data=pd.io.stata.read_stata(\"E:/summer_intern/CAP_alg/data/fertil2.dta\")\n",
    "data = data.fillna(data.mode().iloc[0])\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Save the train_data and test_data to CSV files\n",
    "train_data.to_csv(\"E:/summer_intern/CAP_alg/data/train_data.csv\")\n",
    "test_data.to_csv(\"E:/summer_intern/CAP_alg/data/test_data.csv\")\n",
    "train_data['y'] = train_data['ceb']+train_data['children']\n",
    "test_data['y'] = test_data['ceb']+test_data['children']\n",
    "train_data=train_data.drop(columns=['ceb', 'children'])\n",
    "test_data=test_data.drop(columns=['ceb', 'children'])\n",
    "# Create y column\n",
    "import numpy as np\n",
    "# Merge columns 1 to 8 and 10 into a single column 'x'\n",
    "#import pandas as pd\n",
    "train_data['x'] = train_data.iloc[:, :9].sum(axis=1)\n",
    "test_data['x'] = test_data.iloc[:, :9].sum(axis=1)\n",
    "# Drop the original columns 1 to 8 and 10\n",
    "train_data = train_data.drop(train_data.iloc[:, :9], axis=1)\n",
    "test_data = test_data.drop(test_data.iloc[:, :9], axis=1)\n",
    "\n",
    "train_data['z'] = train_data.iloc[:, :5].sum(axis=1)\n",
    "test_data['z'] = test_data.iloc[:, :5].sum(axis=1)\n",
    "# Drop the original columns 1 to 8 and 10\n",
    "train_data = train_data.drop(train_data.iloc[:, :5], axis=1)\n",
    "test_data = test_data.drop(test_data.iloc[:, :5], axis=1)\n",
    "\n",
    "train_data['a'] = train_data.iloc[:, :5].sum(axis=1)\n",
    "test_data['a'] = test_data.iloc[:, :5].sum(axis=1)\n",
    "# Drop the original columns 1 to 8 and 10\n",
    "train_data = train_data.drop(train_data.iloc[:, :5], axis=1)\n",
    "test_data = test_data.drop(test_data.iloc[:, :5], axis=1)\n",
    "\n",
    "train_data['u'] = train_data.iloc[:, :6].sum(axis=1)\n",
    "test_data['u'] = test_data.iloc[:, :6].sum(axis=1)\n",
    "# Drop the original columns 1 to 8 and 10\n",
    "train_data = train_data.drop(train_data.iloc[:, :6], axis=1)\n",
    "test_data = test_data.drop(test_data.iloc[:, :6], axis=1)\n",
    "\n",
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expectation 0.005668717537983594\n",
      "num_iterations 0\n",
      "num_iterations 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\summer_intern\\CAP_alg\\notebook\\DTR_IV.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/summer_intern/CAP_alg/notebook/DTR_IV.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/summer_intern/CAP_alg/notebook/DTR_IV.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#policy = BootstrappedUCB(LogisticRegression(),10)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/summer_intern/CAP_alg/notebook/DTR_IV.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#print(dataset)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/summer_intern/CAP_alg/notebook/DTR_IV.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/summer_intern/CAP_alg/notebook/DTR_IV.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#print(context_dataset.shape[0])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/summer_intern/CAP_alg/notebook/DTR_IV.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#policy.fit(X=context_dataset,a=np.array(train_data['a']),r=np.array(train_data['y']))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/summer_intern/CAP_alg/notebook/DTR_IV.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m policy\u001b[39m=\u001b[39mCAP_policy_learning(train_data)\n",
      "File \u001b[1;32mE:\\summer_intern/CAP_alg/src\\CAP_algorithm.py:130\u001b[0m, in \u001b[0;36mCAP_policy_learning\u001b[1;34m(dataset, threshold)\u001b[0m\n\u001b[0;32m    127\u001b[0m Set_g\u001b[39m=\u001b[39m[]\n\u001b[0;32m    128\u001b[0m \u001b[39mfor\u001b[39;00m index, data \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m    129\u001b[0m     \u001b[39m#print(data)\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     Set_g\u001b[39m.\u001b[39mappend(CCB_IV(data[\u001b[39m'\u001b[39;49m\u001b[39mz\u001b[39;49m\u001b[39m'\u001b[39;49m],data[\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m],data[\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m],data[\u001b[39m'\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m'\u001b[39;49m],dataset, policy))\n\u001b[0;32m    131\u001b[0m policy\u001b[39m=\u001b[39mLinTS(\u001b[39m10\u001b[39m)\n\u001b[0;32m    132\u001b[0m policy\u001b[39m.\u001b[39mfit(dataset[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m],dataset[\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m],dataset[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mE:\\summer_intern/CAP_alg/src\\Contextualbandit.py:19\u001b[0m, in \u001b[0;36mCCB_IV\u001b[1;34m(Z, A, X, Y, dataset, policy)\u001b[0m\n\u001b[0;32m     17\u001b[0m h1\u001b[39m=\u001b[39mh_func(Y,A,Z)\n\u001b[0;32m     18\u001b[0m fronterior\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m---> 19\u001b[0m frontierior, expectation\u001b[39m=\u001b[39mbinary_search_fronterior(dataset,Z,policy,\u001b[39m0\u001b[39;49m,start_fronterior\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,end_fronterior\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, tol\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(frontierior)\n\u001b[0;32m     21\u001b[0m h11\u001b[39m=\u001b[39mfronterior\u001b[39m+\u001b[39mnp\u001b[39m.\u001b[39msum(Y)\n",
      "File \u001b[1;32mE:\\summer_intern/CAP_alg/src\\observations.py:140\u001b[0m, in \u001b[0;36mbinary_search_fronterior\u001b[1;34m(dataset, Z, policy, target_expectation, start_fronterior, end_fronterior, tol)\u001b[0m\n\u001b[0;32m    138\u001b[0m fronterior \u001b[39m=\u001b[39m (start_fronterior \u001b[39m+\u001b[39m end_fronterior) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[39m# Update the expectation\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m expectation \u001b[39m=\u001b[39m cal_expectation_ob_Z(dataset, Z, fronterior,policy)\n\u001b[0;32m    141\u001b[0m \u001b[39m# Increment the number of iterations\u001b[39;00m\n\u001b[0;32m    142\u001b[0m num_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mE:\\summer_intern/CAP_alg/src\\observations.py:94\u001b[0m, in \u001b[0;36mcal_expectation_ob_Z\u001b[1;34m(dataset, Z, fronterior, policy)\u001b[0m\n\u001b[0;32m     92\u001b[0m countb\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m(countb\u001b[39m<\u001b[39m\u001b[39m10\u001b[39m):\n\u001b[1;32m---> 94\u001b[0m     observational_prob \u001b[39m=\u001b[39m observational_process(row[\u001b[39m'\u001b[39;49m\u001b[39mu\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mz\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m'\u001b[39;49m], policy, dataset)\n\u001b[0;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m observational_prob \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     96\u001b[0m         expectation \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m fronterior \u001b[39m*\u001b[39m observational_prob\n",
      "File \u001b[1;32mE:\\summer_intern/CAP_alg/src\\observations.py:32\u001b[0m, in \u001b[0;36mobservational_process\u001b[1;34m(u, x, z, a, y, policy, dataset)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[39m#print(a)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[39m#print(policy.predict(context))\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m policy\u001b[39m.\u001b[39mpredict(context)\u001b[39m==\u001b[39my:\n\u001b[1;32m---> 32\u001b[0m         context_list\u001b[39m.\u001b[39;49mappend(context)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Problems here:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# No exsiting API or algorithm input function to estimate a posterior probability\u001b[39;00m\n\u001b[0;32m     35\u001b[0m to_count\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mappend(x,z)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'E:/summer_intern/CAP_alg/src')\n",
    "import CAP_algorithm\n",
    "from CAP_algorithm import CAP_policy_learning\n",
    "#print(train_data)\n",
    "#print(train_data['x'].shape[0], train_data['a'].shape[0])\n",
    "from contextualbandits.online import BootstrappedUCB, BootstrappedTS, LinUCB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "#print(dataset)\n",
    "\n",
    "#print(context_dataset.shape[0])\n",
    "#policy.fit(X=context_dataset,a=np.array(train_data['a']),r=np.array(train_data['y']))\n",
    "policy=CAP_policy_learning(train_data)\n",
    "#policy都做的builtin，没有做接口\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use CAP method based on CCB-V model, we exploit the FERTIL2 dataset\n",
    "\n",
    "About FERTIL2 dataset:\n",
    "The goal of this dataset is to study the impact of women's education for more than seven years (or exactly seven) years on the number of children in a household. It contains several observational confounding factors, such as age, television ownership, urban residence, etc. The instrumental variable is a binary indicator that indicates whether a woman was born in the first half of the year. This dataset is often used for the study of tool variables.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
